# 第一周工作总结  

[github](https://github.com/crickwang/Low-Latency-LLM)

---

## 探索  
- 研究了当前最先进的开源模型，包括自动语音识别（ASR）、大语言模型（LLM）和文本转语音（TTS），希望将这些技术结合起来实现语音到语音（speech2speech, s2s）方案。  
- 模型：  
  - **ASR**：Whisper  
  - **LLM**：DeepSeek、Llama 3（本地）、ernie-4.5-0.3b（在线）  
  - **TTS**：Kokoro、Edge_tts  
- 实现了一个基础的语音转文本、用 LLM 处理文本、再将文本转回语音的流式过程。  
  - 该系统目前没有流式处理功能，但基本可以工作。  
  - Whisper 转写 15 秒的音频大约需要 **30 秒**  
  - LLM 处理文本需要约 **3 秒**  
  - TTS 生成语音需要约 **3 秒**  
  - 虽然音质较好，但整体延迟约为 **36 秒**，远远无法满足实时处理的要求。  

### 非流式语音
* 查看非流式语音的实现细节。
```
cd non_streaming_conversation
```
* 运行非流式语音的示例：
```
python conversation.py
```
在程序开始运行后，可以看见程序针对输入的音频文件进行处理，输出转写的文本和生成的语音。

## 挑战  
- 目前实现流式语音的主要问题是 **延迟过高，不适合实时应用**。  
- 对于 TTS：  
  - Kokoro 音质较差  
  - Edge_tts 音质稍好一些，虽然 AI 声音略显机械，但与 Kokoro 相比目前阶段还能接受  
- 对于 ASR：  
  - Whisper 并未针对实时处理优化，转写音频耗时过长  
  - 即使采用并行处理，延迟仍无法满足实时需求  

## 下一步  
- 研究如何 **降低 ASR 模型的延迟**，可能会探索其他模型如 **FunASR**。  
- 寻找 **更高效的 LLM**，以便更快地处理文本，最好是开源方案。  
- 研究 **替代的 TTS 模型**，期望提供更好的音质、更自然的语音以及更低的延迟，可能通过并行处理或使用更小的模型来实现。  
